# -*- coding: utf-8 -*-
"""Fraud_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14K_ugK-QGQwPxxfYXxalRa5wPCnfksy4

# Κατέβασμα dataset και libraries

**Πρέπει να κάνετε upload το αρχείο kaggle.json στον φάκελο /content για να κατεβάσει το dataset**
"""

import os
from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
os.environ['KAGGLE_CONFIG_DIR']='/content'

!kaggle datasets download -d dhanushnarayananr/credit-card-fraud

!unzip /content/credit-card-fraud.zip

!pip install pyspark
import pyspark

pd.read_csv('/content/card_transdata.csv')

"""# Αρχή SparkSession και βασικά στοιχεία του dataset"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('Practise').getOrCreate()

spark

df=spark.read.csv('/content/card_transdata.csv',inferSchema =True,header=True)

df.printSchema()
df.show(truncate=False)

"""Conversion των int values se boolean και το class σε int"""

df = df.selectExpr("cast (distance_from_home as double) distance_from_home",
                    "cast (distance_from_last_transaction as double) distance_from_last_transaction",
                    "cast (ratio_to_median_purchase_price as double) ratio_to_median_purchase_price",
                    "cast(repeat_retailer as boolean) repeat_retailer",
                    "cast(used_chip as boolean) used_chip",
                    "cast(used_pin_number as boolean) used_pin_number",
                    "cast(online_order as boolean) online_order",
                    "cast(fraud as int) fraud")

df.printSchema()
df.show(truncate=False)

"""# Visualisation του Dataset
Χρησιμοποιώντας τα libraries pandas και matplotlib
"""

pd.DataFrame(df.take(5), columns=df.columns).transpose()

df.groupBy('fraud').count().show()

pandas_df = df.toPandas()

colors = ['lightcoral', 'skyblue']
fraud_counts = pandas_df['fraud'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(fraud_counts,colors=colors, labels=fraud_counts.index, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Is the transaction fraudulent?')
plt.suptitle('Class Imbalance')
plt.show()

plt.hist(pandas_df['distance_from_home'], bins=20, edgecolor='black')
plt.xlabel('Distance from Home')
plt.ylabel('Frequency')
plt.title('Distribution of Distance from Home')
plt.show()

plt.hist(pandas_df['distance_from_last_transaction'], bins=20, edgecolor='black')
plt.xlabel('Distance from Last Transaction')
plt.ylabel('Frequency')
plt.title('Distribution of last transaction')
plt.show()

plt.hist(pandas_df['ratio_to_median_purchase_price'], bins=20, edgecolor='black')
plt.xlabel('Ratio to median purchase price')
plt.ylabel('Frequency')
plt.title('Distribution of Ratio to median purchase price')
plt.show()

repeat_retailer = pandas_df['repeat_retailer'].value_counts()
repeat_retailer = repeat_retailer.sort_index(ascending=True)
plt.bar(repeat_retailer.index.astype(str), repeat_retailer, color=colors)
plt.xlabel('repeat_retailer')
plt.ylabel('Count')
plt.title('Distribution of repeat retailer')
for i, v in enumerate(repeat_retailer):
    plt.text(i, v + 10, str(v), color='black', fontweight='bold', ha='center', va='bottom')

plt.show()

used_chip = pandas_df['used_chip'].value_counts()
used_chip = used_chip.sort_index(ascending=True)
plt.bar(used_chip.index.astype(str), used_chip, color=colors)
plt.xlabel('used_chip')
plt.ylabel('Count')
plt.title('Distribution of Used chip')
for i, v in enumerate(used_chip):
    plt.text(i, v+10, str(v), color='black', fontweight='bold', ha='center', va='bottom')
plt.show()

used_pin_number = pandas_df['used_pin_number'].value_counts()
used_pin_number = used_pin_number.sort_index(ascending=True)
plt.bar(used_pin_number.index.astype(str), used_pin_number, color=colors)
plt.xlabel('used_pin_number')
plt.ylabel('Count')
plt.title('Distribution of used pin number')
for i, v in enumerate(used_pin_number):
    plt.text(i, v+10, str(v), color='black', fontweight='bold', ha='center', va='bottom')
plt.show()

online_order = pandas_df['online_order'].value_counts()
online_order = online_order.sort_index(ascending=True)
plt.bar(online_order.index.astype(str), online_order, color=colors)
plt.xlabel('online_order')
plt.ylabel('Count')
plt.title('Distribution of online order')
for i, v in enumerate(online_order):
    plt.text(i, v+10, str(v), color='black', fontweight='bold', ha='center', va='bottom')
plt.show()

corr = pandas_df.corr()
plt.figure(figsize=(15, 8))
sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, annot = True, cmap = "hot")

"""# Data preprocessing
(Όχι ολόκληρο μόνο χωρισμός dataset)

"""

#Duplicate removal (δεν περιέχει το dataset)
df2 = df.dropDuplicates()

"""Tο outlier removal ουσιαστικά μου καταστρέφει το dataset γιατί διαγράφει όλα τα transaction που είναι fraud. Αφαιρέστε τα σύμβολα ''' στην αρχή και στο τέλος για την εκτέλεση.

from pyspark.sql.functions import col


def remove_outliers_iqr(df, columns_to_check):
    filtered_df = df

    for col_name in columns_to_check:
        # Calculate quartiles for the current column
        quantiles = filtered_df.approxQuantile(col_name, [0.25, 0.75], 0.05)  # Adjust relativeError as needed

        # Calculate IQR
        q1 = quantiles[0]
        q3 = quantiles[1]
        iqr = q3 - q1

        # Define the lower and upper bounds to filter outliers
        lower_bound = q1 - (1.5 * iqr)
        upper_bound = q3 + (1.5 * iqr)

        # Filter outliers for the current column
        filtered_df = filtered_df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))

    return filtered_df

# Assuming you have a DataFrame named 'df' with columns 'column1', 'column2', and 'column3'
columns_to_check = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']

# Get the filtered DataFrame with outliers removed
result_filtered_df = remove_outliers_iqr(df, columns_to_check)

# Show the resulting filtered DataFrame
result_filtered_df.show()
result_filtered_df.groupBy('fraud').count().show()

Χωρισμός train set με test set με Stratified splitting
"""

# 'data' represents your original dataset
from pyspark.sql.functions import lit
# Calculate fractions for each stratum
fractions = df.select("fraud").distinct().withColumn("fraction", lit(0.8)).rdd.collectAsMap()

# Perform stratified sampling using sampleBy
training_df = df.sampleBy("fraud", fractions, seed=0)
# Subtract the training data to get the testing data
testing_df = df.subtract(training_df)
training_df.show(truncate=False)
testing_df.show(truncate=False)

training_df.groupBy('fraud').count().show()
testing_df.groupBy('fraud').count().show()

"""# StandardScaler"""

from pyspark.ml.feature import StandardScaler, VectorAssembler


# Assemble features into a vector
assembler = VectorAssembler(inputCols=["distance_from_home", "distance_from_last_transaction", "ratio_to_median_purchase_price"], outputCol="features_vector")
training_df = assembler.transform(training_df)

# Initialize StandardScaler
scaler = StandardScaler(inputCol="features_vector", outputCol="scaled_features", withStd=True, withMean=True)

# generate the scaler model
scaler_model = scaler.fit(training_df)

# scale the features
training_df = scaler_model.transform(training_df)

training_df.show(truncate=False)

"""StandardScaler στο training dataframe"""

# Assemble features into a vector
assembler = VectorAssembler(inputCols=["distance_from_home", "distance_from_last_transaction", "ratio_to_median_purchase_price"], outputCol="features_vector")
testing_df = assembler.transform(testing_df)

# Initialize the StandardScaler
scaler = StandardScaler(inputCol="features_vector", outputCol="scaled_features", withStd=True, withMean=True)

# Compute summary statistics and generate the scaler model
scaler_model = scaler.fit(testing_df)

# Scale the features
testing_df = scaler_model.transform(testing_df)

testing_df.show(truncate=False)

"""# MinMaxScaler
Για τον NaiveBayes

!Aν έχετε κάνει ήδη την StandardScaler ξανατρέξτε την Data preprocessing για να διαγραφtεί η Scaled_features και έπειτα τα υπόλοιπα modules!
"""

from pyspark.ml.feature import MinMaxScaler, VectorAssembler


# Assemble features into a vector
assembler = VectorAssembler(inputCols=["distance_from_home", "distance_from_last_transaction", "ratio_to_median_purchase_price"], outputCol="features_vector")
training_df = assembler.transform(training_df)

# Initialize StandardScaler
scaler = MinMaxScaler(inputCol="features_vector", outputCol="scaled_features")

# generate the scaler model
scaler_model = scaler.fit(training_df)

# scale the features
training_df = scaler_model.transform(training_df)

training_df.show(truncate=False)

from pyspark.ml.feature import MinMaxScaler, VectorAssembler


# Assemble features into a vector
assembler = VectorAssembler(inputCols=["distance_from_home", "distance_from_last_transaction", "ratio_to_median_purchase_price"], outputCol="features_vector")
testing_df = assembler.transform(testing_df)

# Initialize StandardScaler
scaler = MinMaxScaler(inputCol="features_vector", outputCol="scaled_features")

# generate the scaler model
scaler_model = scaler.fit(testing_df)

# scale the features
testing_df = scaler_model.transform(testing_df)

testing_df.show(truncate=False)

"""# Oversampling
Διαλέγετε oversampling ή undersampling

Το τελικο dataset είναι το balanced_df

Προσπάθησα να κάνω oversample με την τεχνική του SMOTE μετατρέποντας το dataframe σε pandas πρώτα αλλά εν τέλη δεν μου δούλευε οπότε αποφάσησα να το κάνω απλά με το sampleBy.

from imblearn.over_sampling import SMOTE

pandas_scaled = scaled_data.select("fraud", "scaled_features").toPandas()
X = pandas_scaled["scaled_features"]
y = pandas_scaled["fraud"]

smote = SMOTE(sampling_strategy = 'minority', random_state = 42)

X_resampled, y_resampled = smote.fit_resample(X, y)

balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=["distance_from_home ", "distance_from_last_transaction","ratio_to_median_purchase_price","repeat_retailer","used_chip","used_pin_number","online_order"]), pd.DataFrame(y_resampled, columns=["label"])], axis=1)
balanced_df = spark.createDataFrame(balanced_pandas_df)
"""

# Πηγή https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253
# Το dataset δεν είναι ακριβώς 50-50 αλλά οι άλλες τεχνικές που βρίκα για παράδειγμα με το sampleBy() και sample() δεν κατάφερα να τις κάνω να δουλέψουν
# https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html

from pyspark.sql.functions import col, explode, array, lit

major_df = training_df.filter(col('fraud') == 0)
minor_df = training_df.filter(col('fraud') == 1)

ratio = int(major_df.count()/minor_df.count())
print("ratio: {}".format(ratio))
a = range(ratio)
# duplicate the minority rows
oversampled = minor_df.withColumn("dummy", explode(array([lit(x) for x in a]))).drop('dummy')
# combine both oversampled minority rows and previous majority rows
balanced_df = major_df.unionAll(oversampled)
balanced_df.show()

balanced_df.groupBy('fraud').count().show()

"""# Undersampling"""

# Πηγή https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253
# Το dataset δεν είναι ακριβώς 50-50 αλλά οι άλλες τεχνικές που βρίκα για παράδειγμα με το sampleBy() και sample() δεν κατάφερα να τις κάνω να δουλέψουν
# https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html

from pyspark.sql.functions import col

major_df = training_df.filter(col('fraud') == 0)
minor_df = training_df.filter(col('fraud') == 1)
ratio = int(major_df.count()/minor_df.count())

sampled_majority_df = major_df.sample(False, 1/ratio)
balanced_df = sampled_majority_df.unionAll(minor_df)

balanced_df.show()

balanced_df.groupBy('fraud').count().show()

"""# Assemplers

Για training_df
"""

from pyspark.ml.feature import VectorAssembler

#Τα κάνω όλα asseble σε ένα column
assembler = VectorAssembler(inputCols=[
    'scaled_features', 'repeat_retailer', 'used_chip',
    'used_pin_number',
    'online_order'
                  ], outputCol='features')

# Consolidate predictor columns
assempled_df = assembler.transform(balanced_df)

assempled_df.select('features', 'fraud').show(truncate=False)

"""Για testing_df"""

from pyspark.ml.feature import VectorAssembler

#Τα κάνω όλα asseble σε ένα column
assembler = VectorAssembler(inputCols=[
    'scaled_features', 'repeat_retailer', 'used_chip',
    'used_pin_number',
    'online_order'
                  ], outputCol='features')

# Consolidate predictor columns
assempled_test_df = assembler.transform(testing_df)

assempled_test_df.select('features', 'fraud').show(truncate=False)

"""# Classification Algorithms
Μην τρέξετε την όλη την ενότητα. Διαλέξτε πιον αλγόριθμο θέλετε και έπειτα το Evaluation.

Decision tree
"""

from pyspark.ml.classification import DecisionTreeClassifier

tree = DecisionTreeClassifier(labelCol="fraud", featuresCol="features")
tree_model = tree.fit(assempled_df)
prediction = tree_model.transform(assempled_test_df)

print (tree_model.toDebugString)

"""Random Forest


"""

from pyspark.ml.classification import RandomForestClassifier

rf = RandomForestClassifier(labelCol="fraud", featuresCol="features")
rf_model = rf.fit(assempled_df)
prediction = rf_model.transform(assempled_test_df)

print (rf_model.toDebugString)

"""Naive Bayes"""

from pyspark.ml.classification import NaiveBayes

assempled_df.select('features', 'fraud').show(truncate=False)

nb = NaiveBayes(labelCol="fraud", featuresCol="features",modelType="multinomial")
nb_model = nb.fit(assempled_df)
prediction = nb_model.transform(assempled_test_df)

"""# Evaluation"""

# take a look at the predictions
prediction.select('fraud', 'prediction', 'probability').show(5, False)

#Πηγή https://goodboychan.github.io/python/datacamp/pyspark/machine_learning/2020/08/10/03-Classification-in-PySpark.html#Assembling-columns
prediction.groupBy('fraud', 'prediction').count().show()

# Calculate the elements of the confusion matrix
TN = prediction.filter('prediction = 0 AND fraud = prediction').count()
TP = prediction.filter('prediction = 1 AND fraud = prediction').count()
FN = prediction.filter('prediction = 0 AND fraud = 1').count()
FP = prediction.filter('prediction = 1 AND fraud = 0').count()

# Accuracy measures the proportion of correct predictions
accuracy = (TN + TP) / (TN + TP + FN + FP)
print(accuracy)

print("TN =",TN)
print("TP =",TP)
print("FN =",FN)
print("FP =",FP)

print("Accuracy =", (TP + TN) / (TP + TN + FP + FN))
print("Misclassification =", (FP + FN) / (TP + TN + FP + FN))
print("Precision=", TP / (TP + FP))
print("Sensitivity or Recall =", TP / (TP + FN))
print("Specificity =",TN / (TN + FP))

from sklearn.metrics import confusion_matrix

#  [[true_negatives, false_positives],
#  [false_negatives, true_positives]]
conf_matrix_values = [[TN, FP],
                      [FN, TP]]
sns.set(font_scale=1.4)
plt.figure(figsize=(10, 5))
sns.heatmap(conf_matrix_values, annot=True, fmt='g', cmap='Blues',
            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()